#
import os
import warnings
warnings.filterwarnings("ignore")

from sklearn import metrics
import lazypredict
from lazypredict.Supervised import LazyClassifier

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

from sklearn import preprocessing #This for Data Rescaling (Normalization and Standardization)

from sklearn.ensemble import ExtraTreesClassifier
import lightgbm as lgb
from lightgbm import LGBMClassifier
from sklearn.semi_supervised import LabelPropagation
from sklearn.semi_supervised import LabelSpreading
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
from sklearn.model_selection import learning_curve

from sklearn.preprocessing import PowerTransformer

from xgboost import XGBClassifier

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score

from matplotlib import pyplot
from pandas import read_csv
import pandas as pd
import numpy as np

# load the two datasets
#datasetProcess = read_csv('setapProcessT1-T11.csv', header=None)
datasetProcess = read_csv('NEW3_permissions_and_intent_vectors_raw_csv.csv', delimiter=';')

# determine the number of input features (columns)
n_features_Process = datasetProcess.shape[1]
print('Features Process:', n_features_Process)

# split into input (X) and output (y) variables
XProcess = datasetProcess.drop(['is_malware'], axis=1)
yProcess = datasetProcess['is_malware']

#The below if we want to change the columns description with numbers, since if we want to use .hist method this is needed.
new_XProcess = XProcess

idx = []
for i in range(1, 1004):
   idx.insert(len(idx), i)     
new_XProcess.columns = idx

#Make histogram of all features
#new_XProcess.hist(figsize=(26, 26))

#Make histogram of specific features
new_XProcess[[4, 5, 8, 18, 19, 26, 67, 76, 77]].hist(figsize=(20, 20))
pyplot.show()

pyplot.clf()
new_XProcess[[4, 5, 8, 18, 19, 26, 67, 76, 77]].boxplot()
#XProcess.boxplot()
pyplot.show()

pd.set_option('display.max_rows', None, 'display.max_columns', None) #This is for configuring panda dataframe not to truncate the values when it display them
print("VALUE ANALYSIS of the input FEATURES (count, mean, std, min, max, ....)")
print(XProcess.describe().T.round(2))

# Standardize the data attributes with PowerTransformer (this is a logarithmic scaler
# Init
pt = PowerTransformer()
standardized_XProcess = pd.DataFrame(pt.fit_transform(XProcess))
standardized_XProcess[[4, 5, 8, 18, 19, 26, 67, 76, 77]].hist(figsize=(20, 20))

#Beware, that if i use the above standardized_XProcess, then the XGBoost performs worst than some of the other ML algorithms.

# normalize the data attributes of XProcess
normalized_standardized_XProcess = preprocessing.normalize(standardized_XProcess)
pd.DataFrame(normalized_standardized_XProcess)[[4, 5, 8, 18, 19, 26, 67, 76, 77]].hist(figsize=(20, 20))
pyplot.show()
#print(type(normalized_XProcess))

# encode strings to integer
yProcess = LabelEncoder().fit_transform(yProcess)

# split into train and test datasets
X_Process_train, X_Process_test, y_Process_train, y_Process_test = train_test_split(normalized_standardized_XProcess, yProcess, train_size=0.80, test_size=0.20)

clf = LazyClassifier(verbose=0, ignore_warnings=True) 
models, predictions = clf.fit(X_Process_train, X_Process_test, y_Process_train, y_Process_test) 
print(models)

#Start evaluating different ML models to check the accuracy prediction. The selection of the models depend on the results derived
# from the previous phase of the research.

#------------------------------------------------------------------
print('-----------------------------------')
print('Evaluate the LGBMClassifier')
print('-----------------------------------')
# Site--> https://machinelearningmastery.com/extra-trees-ensemble-with-python/
model = LGBMClassifier()
model.fit(X_Process_train, y_Process_train, eval_metric=["auc","rmse", "logloss", "error"])
predictions_y = model.predict(X_Process_test)

# summarize the first 10 cases
for i in range(10):
#       print('%s => %d (expected %d)' % (X_Process_test[i].tolist(), predictions[i], y_Process_test[i]))
        print('%d (expected %d)' % (predictions_y[i], y_Process_test[i]))

print('----------- Classification results ------------------------')
print(metrics.classification_report(y_Process_test, predictions_y))
print('----------- End of Classification summary results ---------')

print('VARIOUS EVALUATION METRICS of LGBMClassifier')
print('---------------------------------------------------------------')
# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_Process_test, predictions_y)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_Process_test, predictions_y)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_Process_test, predictions_y)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_Process_test, predictions_y)
print('F1 score: %f' % f1)
 
# kappa
kappa = cohen_kappa_score(y_Process_test, predictions_y)
print('Cohens kappa: %f' % kappa)

# ROC AUC
auc = roc_auc_score(y_Process_test, predictions_y)
print('ROC AUC: %f' % auc)

#print('Confusion Matrix')
matrix = confusion_matrix(y_Process_test, predictions_y)
print(matrix)

pyplot.imshow(matrix, cmap=pyplot.cm.hot)
pyplot.xlabel("Predicted labels")
pyplot.ylabel("True labels")
pyplot.xticks([], [])
pyplot.yticks([], [])
pyplot.title('Confusion matrix ')
pyplot.colorbar()
pyplot.show()

print('###################################')

#print('###################################')

#------------------------------------------------------------------
print('Starting the evaluation')
print('-----------------------------------')
print('Evaluate the XGBClassifier')
print('-----------------------------------')
model = XGBClassifier()
model.fit(X_Process_train, y_Process_train, eval_set=[(X_Process_train, y_Process_train), (X_Process_test, y_Process_test)], eval_metric=["auc", "rmse", "logloss", "error"])
predictions_y = model.predict(X_Process_test)

# summarize the first 10 cases
for i in range(10):
#       print('%s => %d (expected %d)' % (X_Process_test[i].tolist(), predictions[i], y_Process_test[i]))
        print('%d (expected %d)' % (predictions_y[i], y_Process_test[i]))

print('----------- Classification results ------------------------')
print(metrics.classification_report(y_Process_test, predictions_y))
print('----------- End of Classification summary results ---------')

print('VARIOUS EVALUATION METRICS of XGBClassifier')
print('---------------------------------------------------------------')
# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_Process_test, predictions_y)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_Process_test, predictions_y)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_Process_test, predictions_y)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_Process_test, predictions_y)
print('F1 score: %f' % f1)
 
# kappa
kappa = cohen_kappa_score(y_Process_test, predictions_y)
print('Cohens kappa: %f' % kappa)

# ROC AUC
auc = roc_auc_score(y_Process_test, predictions_y)
print('ROC AUC: %f' % auc)

print('Confusion Matrix')
matrix = confusion_matrix(y_Process_test, predictions_y)
print(matrix)

pyplot.imshow(matrix, cmap=pyplot.cm.hot)
pyplot.xlabel("Predicted labels")
pyplot.ylabel("True labels")
pyplot.xticks([], [])
pyplot.yticks([], [])
pyplot.title('Confusion matrix ')
pyplot.colorbar()
pyplot.show()

#Plot the Error and loss for the model
results = model.evals_result()
epochs = len(results['validation_0']['error'])
x_axis = range(0, epochs)
# plot log loss
fig, ax = pyplot.subplots(figsize=(12,12))
ax.plot(x_axis, results['validation_0']['logloss'], label='Train')
ax.plot(x_axis, results['validation_1']['logloss'], label='Test')
ax.legend()
pyplot.ylabel('Log Loss')
pyplot.title('XGBoost Log Loss (negative log-likelihood)')
pyplot.show()

# plot classification error
fig, ax = pyplot.subplots(figsize=(12,12))
ax.plot(x_axis, results['validation_0']['error'], label='Train')
ax.plot(x_axis, results['validation_1']['error'], label='Test')
ax.legend()
pyplot.ylabel('Classification Error')
pyplot.title('XGBoost Classification Error')
pyplot.show()

# plot root mean square error (RMSE)
fig, ax = pyplot.subplots(figsize=(12,12))
ax.plot(x_axis, results['validation_0']['rmse'], label='Train')
ax.plot(x_axis, results['validation_1']['rmse'], label='Test')
ax.legend()
pyplot.ylabel('Root Mean Square Error(RMSE)')
pyplot.title('XGBoost RMSE')
pyplot.show()

# if we want to change default figure and font size
pyplot.rcParams['figure.figsize'] = 8, 6 
pyplot.rcParams['font.size'] = 12

x_axis = range(len(results['validation_0']['auc']))
pyplot.plot(x_axis, results['validation_0']['auc'], label = 'Train')
pyplot.plot(x_axis, results['validation_1']['auc'], label = 'Test')
pyplot.legend(loc = 'best')
pyplot.ylabel('AUC')
pyplot.title('Xgboost AUC')
pyplot.show()

print('###################################')

# Below you may evaluate other ML models by uncommented the commands

#------------------------------------------------------------------
#print('-----------------------------------')
#print('Evaluate the LabelSpreading')
#print('-----------------------------------')
#model_eval = LabelSpreading()
#model_eval.fit(X_Process_train, y_Process_train)
#predictions_y = model_eval.predict(X_Process_test)

# summarize the first 10 cases
#for i in range(10):
##       print('%s => %d (expected %d)' % (X_Process_test[i].tolist(), predictions[i], y_Process_test[i]))
#        print('%d (expected %d)' % (predictions_y[i], y_Process_test[i]))

#print('----------- Classification results ------------------------')
#print(metrics.classification_report(y_Process_test, predictions_y))
#print('----------- End of Classification summary results ---------')

#print('VARIOUS EVALUATION METRICS of LabelSpreading')
#print('---------------------------------------------------------------')
## accuracy: (tp + tn) / (p + n)
#accuracy = accuracy_score(y_Process_test, predictions_y)
#print('Accuracy: %f' % accuracy)
## precision tp / (tp + fp)
#precision = precision_score(y_Process_test, predictions_y)
#print('Precision: %f' % precision)
## recall: tp / (tp + fn)
#recall = recall_score(y_Process_test, predictions_y)
#print('Recall: %f' % recall)
## f1: 2 tp / (2 tp + fp + fn)
#f1 = f1_score(y_Process_test, predictions_y)
#print('F1 score: %f' % f1)
 
# kappa
#kappa = cohen_kappa_score(y_Process_test, predictions_y)
#print('Cohens kappa: %f' % kappa)

# ROC AUC
#auc = roc_auc_score(y_Process_test, predictions_y)
#print('ROC AUC: %f' % auc)

#print('Confusion Matrix')
#matrix = confusion_matrix(y_Process_test, predictions_y)
#print(matrix)

#pyplot.imshow(matrix, cmap=pyplot.cm.hot)
#pyplot.xlabel("Predicted labels")
#pyplot.ylabel("True labels")
#pyplot.xticks([], [])
#pyplot.yticks([], [])
#pyplot.title('Confusion matrix ')
#pyplot.colorbar()
#pyplot.show()

#print('###################################')

#------------------------------------------------------------------
#print('-----------------------------------')
#print('Evaluate the QuadraticDiscriminantAnalysis')
#print('-----------------------------------')
#model_eval = QuadraticDiscriminantAnalysis()
#model_eval.fit(X_Process_train, y_Process_train)
#predictions_y = model_eval.predict(X_Process_test)

# summarize the first 10 cases
#for i in range(10):
##       print('%s => %d (expected %d)' % (X_Process_test[i].tolist(), predictions[i], y_Process_test[i]))
#        print('%d (expected %d)' % (predictions_y[i], y_Process_test[i]))

#print('----------- Classification results ------------------------')
#print(metrics.classification_report(y_Process_test, predictions_y))
#print('----------- End of Classification summary results ---------')

#print('VARIOUS EVALUATION METRICS of QuadraticDiscriminantAnalysis')
#print('---------------------------------------------------------------')
## accuracy: (tp + tn) / (p + n)
#accuracy = accuracy_score(y_Process_test, predictions_y)
#print('Accuracy: %f' % accuracy)
## precision tp / (tp + fp)
#precision = precision_score(y_Process_test, predictions_y)
#print('Precision: %f' % precision)
## recall: tp / (tp + fn)
#recall = recall_score(y_Process_test, predictions_y)
#print('Recall: %f' % recall)
## f1: 2 tp / (2 tp + fp + fn)
#f1 = f1_score(y_Process_test, predictions_y)
#print('F1 score: %f' % f1)
 
# kappa
#kappa = cohen_kappa_score(y_Process_test, predictions_y)
#print('Cohens kappa: %f' % kappa)

# ROC AUC
#auc = roc_auc_score(y_Process_test, predictions_y)
#print('ROC AUC: %f' % auc)

#print('Confusion Matrix')
#matrix = confusion_matrix(y_Process_test, predictions_y)
#print(matrix)

#pyplot.imshow(matrix, cmap=pyplot.cm.hot)
#pyplot.xlabel("Predicted labels")
#pyplot.ylabel("True labels")
#pyplot.xticks([], [])
#pyplot.yticks([], [])
#pyplot.title('Confusion matrix ')
#pyplot.colorbar()
#pyplot.show()

#print('###################################')


