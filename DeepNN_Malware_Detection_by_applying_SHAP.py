# Article for DNN

import os
import numpy as np
import numpy as np_weights

import shap

from numpy import genfromtxt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

from tensorflow.keras.layers import BatchNormalization

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.layers import Dropout

from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.optimizers import Nadam
from tensorflow.keras.optimizers import Ftrl

from tensorflow.keras.utils import plot_model
from matplotlib import pyplot

from tensorflow.keras.models import load_model


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score

from pandas import read_csv
import pandas as pd

import tensorflow as tf
import tensorflow.keras as k

#The explainer in new version of shap was moved and imported as below
from shap import Explainer

from tensorflow.keras.callbacks import TensorBoard
from time import time

dataset = read_csv('NEW3_permissions_and_intent_vectors_raw_csv.csv', delimiter=';')

# print the field names with one of the two below methods
print('Field names:', dataset.columns)
#print('Field names:', dataset.keys())

# determine the number of input features (columns)
n_features = dataset.shape[1]
print('Number of Features(columns):', n_features)
print('Number of records(lines):', len(dataset))

# split into input (X) and output (y) variables
# the : means --> from the bigging or all
XProcess = dataset.values[:,:-1]
yProcess = dataset.values[:,-1] #if we use numpy all the chars return NaN (not a number)

# encode strings to integer
yProcess = LabelEncoder().fit_transform(yProcess)

# split into train and test datasets
# If only train_size is set, then test_size is the complement of train_size
# If only test_size is set, then train_size is the compliment of test_size
# If NONE of train_size and test_size are set, then test_size is set to 0.25 (and 0.75 the train_size accordingly)
X_Process_train, X_Process_test, y_Process_train, y_Process_test = train_test_split(XProcess, yProcess, train_size=0.80, test_size=0.20)

print("=====================New Dataset shape after the spliting============================")
print('X_train, X_test, Y_traing, Y_test:', X_Process_train.shape, X_Process_test.shape, y_Process_train.shape, y_Process_test.shape)

# MACHINE LEARNING MODEL

# define the keras DL model
model = Sequential()

#Choose the structure and test

#model.add(Dense(14, input_dim=n_features_Process-1, activation='sigmoid'))
#model.add(Dense(7, activation='sigmoid'))
#model.add(Dense(1, activation='sigmoid'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='tanh'))
#model.add(Dense(7, activation='tanh'))
#model.add(Dense(1, activation='tanh'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='relu'))
#model.add(Dense(7, activation='relu'))
#model.add(Dense(1, activation='relu'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='sigmoid'))
#model.add(Dense(7, activation='sigmoid'))
#model.add(Dense(1, activation='relu'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='sigmoid'))
#model.add(Dense(7, activation='sigmoid'))
#model.add(Dense(1, activation='tanh'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='relu'))
#model.add(Dense(7, activation='relu'))
#model.add(Dense(1, activation='tanh'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='tanh'))
#model.add(Dense(7, activation='tanh'))
#model.add(Dense(1, activation='sigmoid'))

#model.add(Dense(14, input_dim=n_features_Process-1, activation='tanh'))
#model.add(Dense(7, activation='tanh'))
#model.add(Dense(1, activation='relu'))

#model.add(Dense(1004, input_dim=n_features-1, activation='sigmoid'))
#model.add(Dense(502, activation='sigmoid'))
#model.add(Dense(256, activation='sigmoid'))
#model.add(Dense(128, activation='sigmoid'))
#model.add(Dense(64, activation='sigmoid'))
#model.add(Dense(1, activation='sigmoid'))

#With 7 layers
##model.add(Dense(284, input_dim=n_features-1, activation='sigmoid'))
##model.add(Dropout(0.42004354788351883))
##model.add(Dense(646, activation='sigmoid'))
##model.add(Dropout(0.2748214346378275))
##model.add(Dense(612, activation='sigmoid'))
##model.add(Dropout(0.2522491220677714))
##model.add(Dense(334, activation='sigmoid'))
##model.add(Dropout(0.3815409886152511))
##model.add(Dense(234, activation='sigmoid'))
##model.add(Dropout(0.3815409886152511))
##model.add(Dense(134, activation='sigmoid'))
##model.add(Dropout(0.3815409886152511))
##model.add(Dense(1, activation='sigmoid'))

#With 4 layres as shown in Optuna
#model.add(Dense(284, input_dim=n_features-1, activation='sigmoid'))
#model.add(Dropout(0.42004354788351883))
#model.add(Dense(646, activation='sigmoid'))
#model.add(Dropout(0.2748214346378275))
#model.add(Dense(612, activation='sigmoid'))
#model.add(Dropout(0.2522491220677714))
#model.add(Dense(334, activation='sigmoid'))
#model.add(Dropout(0.3815409886152511))
#model.add(Dense(1, activation='sigmoid'))

#With 3 layres as shown in Optuna
model.add(Dense(238, input_dim=n_features-1, activation='sigmoid'))
model.add(Dropout(0.22363812951389536))
model.add(Dense(91, activation='sigmoid'))
model.add(Dropout(0.35457281339635777))
model.add(Dense(427, activation='sigmoid'))
model.add(Dropout(0.3784279030594993))
model.add(Dense(1, activation='sigmoid'))

#model.add(Dense(1, activation='softsign'))

# summarize the model
model.summary()

# Create a plot of the NN model
plot_model(model, 'model_process.png', show_shapes=True)


# Compile the keras NN model
print('Starting compiling the NN model')
print('#####################################')
tf.keras.backend.clear_session()

#Select the relevant model

#model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adagrad(), metrics=['accuracy'])
#model.compile(loss='binary_crossentropy', optimizer='Adagrad', metrics=['accuracy'])
#model.compile(loss='binary_crossentropy', optimizer='adadelta', metrics=['accuracy'])
#model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) #NOT VALID COMBINATION MAYBE NEED SOMETHING ELSE IN DATA
#model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.RMSprop(), metrics=['accuracy'])


#model.compile(loss='binary_crossentropy', optimizer='RMSprop', metrics=['accuracy'])
#model.compile(loss='mean_squared_error', optimizer='RMSprop', metrics=['accuracy'])

#model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
#model.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])
model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adamax(), metrics=['accuracy'])

# Loss = Probabilistic losses
# Select the preferred loss function

#model.compile(loss='binary_crossentropy', optimizer='Nadam', metrics=['accuracy'])
#model.compile(loss='categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])
#model.compile(loss='mean_squared_error', optimizer='Nadam', metrics=['accuracy'])
#model.compile(loss='kullback_leibler_divergence', optimizer='Nadam', metrics=['accuracy'])
#model.compile(loss='sparse_categorical_crossentropy', optimizer='Nadam', metrics=['accuracy'])
#model.compile(loss='kl_divergence', optimizer='Nadam', metrics=['accuracy'])

#model.compile(loss='binary_crossentropy', optimizer='Ftrl', metrics=['accuracy'])

#model.compile(loss=tf.keras.losses.binary_crossentropy, optimizer=tf.keras.optimizers.Adadelta(), metrics=['accuracy'])
#model.compile(loss='mean_squared_error', optimizer='adadelta', metrics=['accuracy'])
#model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy'])
#model.compile(loss='mean_squared_error', optimizer='RMSprop', metrics=['accuracy'])
#model.compile(loss='mean_squared_error', optimizer='Adagrad', metrics=['accuracy']) # MAKE NO PREDICTION WITH PRODUCT DATA. EVERYTHING ARE PREDICTED AS 0
#model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy']) # MAKE NO PREDICTION WITH PRODUCT DATA. EVERYTHING ARE PREDICTED AS 0
#model.compile(loss='mean_squared_error', optimizer='adamax', metrics=['accuracy']) #BAD PREDICTIONS WITH PRODUCT DATA

#model.compile(loss='kullback_leibler_divergence', optimizer='adamax', metrics=['accuracy'])

print('Starting the Training phase of the NN')
print('#####################################')

# patient early stopping
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)

# create and save the training model with a file name --> best_model.h5, that contains the best model up to the current point
mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)


# Fit the keras model on the dataset and start training
h_Process = model.fit(X_Process_train, y_Process_train, epochs=1000, batch_size=50, verbose=0, validation_split=0.2, callbacks=[es, mc])

#Print the weights for each layer
np_weights = model.layers[0].get_weights()
print('####################################')
print('Weights in layer 0 (Input layer)')
print(np_weights)

print("%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%")
print(np_weights[0][0])

np_weights = model.layers[1].get_weights()
print('####################################')
print('Weights in layer 1 (1st hidden layer)')
print(np_weights)

np_weights = model.layers[2].get_weights()
print('####################################')
print('Weights in layer 2 (2nd hidden layer)')
print(np_weights)

#Start ploting

# Plot the necessary learning curves
pyplot.title('Learning Curves')
pyplot.xlabel('Epoch')
pyplot.ylabel('Cross Entropy (Loss)')
pyplot.plot(h_Process.history['loss'], label='train')
pyplot.plot(h_Process.history['val_loss'], label='val')
pyplot.legend()
pyplot.show()

# Plot the necessary accuracy learning curves
pyplot.title('Accuracy Learning Curves')
pyplot.xlabel('Epoch')
pyplot.ylabel('Accuracy')
pyplot.plot(h_Process.history['accuracy'], label='Train')
pyplot.plot(h_Process.history['val_accuracy'], label='Val')
pyplot.legend()
pyplot.show()

#ANOTHER way to plot

training_loss = h_Process.history['loss']
test_loss = h_Process.history['val_loss']

# Create count of the number of epochs
epoch_count = range(1, len(training_loss) + 1)

# Visualize loss history

pyplot.plot(epoch_count, training_loss, 'r--')
pyplot.plot(epoch_count, test_loss, 'b-')
pyplot.legend(['Training Loss', 'Val Loss'])
pyplot.xlabel('Epoch')
pyplot.ylabel('Loss')
pyplot.show();

np.set_printoptions(precision=3)

print('Starting displaying the values of the NN for each EPOCH')
print('#####################################')
print('--------During Training phase the loss values ---------')
#print('Training loss:', h.history['loss'])
r1 = np.array(h_Process.history['loss'])
print('----------- Number of values %d ----------' % r1.size)
print(r1)

print()
print('------ During Training phase the accuracy values ------')
#print('Training accuracy:', h.history['accuracy'])
r2 = np.array(h_Process.history['accuracy'])
print('----------- Number of values %d ----------' % r2.size)
print(r2)

print()
print('------ During Validation phase the loss values --------')
#print('During Validation -->loss:', h.history['val_loss'],)
r3 = np.array(h_Process.history['val_loss'])
print('----------- Number of values %d ----------' % r3.size)
print(r3)

print()
print('------ During Validation phase the accuracy values ----')
#print('Validation accuracy:', h.history['val_accuracy'])
r4 = np.array(h_Process.history['val_accuracy'])
print('----------- Number of values %d ----------' % r4.size)
print(r4)

print('------- END of TRAINING and VALIDATION phase ----------')

# SECOND: The below is an alternative display of ACCURACY.
# After the training, we use ALL the data to evaluate the model
# That is because the final result of accuracy with this method it is different than the result compared to the last value of the above Accuracy argument (FIRST method)

print('------- STARTING EVALUATION phase WITH ALL THE DATA -------')

#filepath = '/Temp/Security/'
#tf.keras.models.save_model(model, filepath, overwrite=True, include_optimizer=True, save_format='tf', signatures=None, options=None)
#tfmodel = tf.saved_model.load(filepath, tags=None)
model = load_model('best_model.h5')

# we use the first 100 training examples as our background dataset to integrate over
# Working the below
#-----------------------
background = np.asarray(X_Process_train[np.random.choice(X_Process_train.shape[0], 100, replace=False)]).astype(np.float32)

#### shap_values = explainer.shap_values(np.asarray(X_Process_test[1:30]).astype(np.float32))
#shap_values = explainer.shap_values(np.asarray(X_Process_test).astype(np.float32))
explainer = shap.DeepExplainer(model, background)
shap_values = explainer.shap_values(X_Process_test)
# load JS visualization code to notebook
#shap.initjs()

#summarize the effects of all features
####shap.summary_plot(shap_values, X_Process_test[1:30], plot_type="bar")
shap.summary_plot(shap_values, X_Process_test, feature_names = dataset.columns)
#shap.summary_plot(shap_values, X_Process_test)
#shap.summary_plot(shap_values[0], X_Process_test)
shap.summary_plot(shap_values[0], X_Process_test, feature_names = dataset.columns)


#shap.summary_plot(shap_values, X_Process_test, plot_type='bar')

# The first argument is the index of the feature we want to plot
# The second argument is the matrix of SHAP values (it is the same shape as the data matrix)
# The third argument is the data matrix (a pandas dataframe or numpy array)
shap.dependence_plot(64, shap_values[0], X_Process_test, show=False)
pyplot.title("Feature 64 dependence plot")
pyplot.ylabel("SHAP value for the 'Feature64' feature")
pyplot.show()

#Plot the SHAP values for one instance
shap.initjs()

X_Process_test_words = np.stack([np.array(list(map(lambda x: dataset.get(x, "NONE"), X_Process_test[i]))) for i in range(10)])

# evaluate the keras model
score, accuracy = model.evaluate(np.asarray(X_Process_train).astype(np.float32), y_Process_train)
print('Accuracy: %.2f' % (accuracy*100))
print('-------------------------------------------------------')

print('Score: %.2f' % (score*100))
print('-------------------------------------------------------')

#--------------------------------------
# make class predictions with the model
print('Starting the Prediction phase')
print('#####################################')
predictions = model.predict_classes(np.asarray(X_Process_test).astype(np.float32))

# predict probabilities for test set
predictions_probs = model.predict(np.asarray(X_Process_test).astype(np.float32))

print('---------- A sample of 10 prediction ------------')
# summarize the first 10 cases
for i in range(10):
	print('%d (expected %d)' % (predictions[i], y_Process_test[i]))

print('EVALUATION METRICS with the test sample and prediction data')
print('---------------------------------------------------------------')
# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_Process_test, predictions[:, 0])
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_Process_test, predictions[:, 0])
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_Process_test, predictions[:, 0])
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_Process_test, predictions[:, 0])
print('F1 score: %f' % f1)
 
# kappa
kappa = cohen_kappa_score(y_Process_test, predictions[:, 0])
print('Cohens kappa: %f' % kappa)

# ROC AUC
auc = roc_auc_score(y_Process_test, predictions_probs[:, 0])
print('ROC AUC: %f' % auc)

print('Confusion Matrix')
matrix = confusion_matrix(y_Process_test, predictions[:, 0])
print(matrix)

pyplot.imshow(matrix, cmap=pyplot.cm.hot)
pyplot.xlabel("Predicted labels")
pyplot.ylabel("True labels")
pyplot.xticks([], [])
pyplot.yticks([], [])
pyplot.title('Confusion matrix ')
pyplot.colorbar()
pyplot.show()

print('###################################')
print(f'Shape of dataset: {X_Process_test.shape}')
print(f'Type of shap_values: {type(shap_values)}. Length of the list: {len(shap_values)}')
print(f'Shape of shap_values: {np.array(shap_values).shape}')
print('###################################')
