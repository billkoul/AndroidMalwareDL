#
import os
import warnings
warnings.filterwarnings("ignore")

import xgboost as xgb
from xgboost.sklearn import XGBClassifier
import numpy as np
import pandas as pd

import optuna
from optuna.visualization.matplotlib import plot_optimization_history
from optuna.integration import KerasPruningCallback
from optuna.trial import TrialState

from matplotlib import pyplot

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score

from pandas import read_csv


dataset = read_csv('NEW3_permissions_and_intent_vectors_raw_csv.csv', delimiter=';')

# determine the number of input features (columns)
n_features = dataset.shape[1]
print('Number of Features(columns):', n_features)
print('Number of records(lines):', len(dataset))

# split into input (X) and output (y) features
XDataSet = dataset.drop(['is_malware'], axis=1)
yDataset = dataset['is_malware']

#Split the data
X_train, X_test, y_train, y_test = train_test_split(XDataSet, yDataset, train_size=0.80, test_size=0.20)

print("DATA")
print("Train X data shape: ", X_train.shape)
print("Validation X data shape: ", X_test.shape)

print("Train Y data shape: ", y_train.shape)
print("Validation Y data shape: ", y_test.shape)


# MACHINE LEARNING MODEL

model = XGBClassifier()
h_Process= model.fit(X_train, y_train,  eval_set=[(X_train, y_train), (X_test, y_test)], eval_metric=["auc","rmse", "logloss", "error"])
# retrieve performance metrics
results = model.evals_result()
epochs = len(results['validation_0']['error'])
x_axis = range(0, epochs)

print('Starting the Training phase of the XGboost')
print('#####################################')

# The below set precision = 3 so the below list values that contained in r1, r2, r3, and r4 to have up to 3 decimals.
np.set_printoptions(precision=3)

print('------- STARTING EVALUATION phase WITH ALL THE DATA -------')

y_pred = model.predict(X_train)
predictions = [round(value) for value in y_pred]
accuracy = accuracy_score(y_train, predictions)
print("Train data Accuracy: %.2f%%" % (accuracy * 100.0))

#Additonally, evaluate XGBoost Models With k-Fold Cross Validation
kfold = KFold(n_splits=10)
#kfold = KFold(n_splits=10, shuffle=True, random_state=7)
results = cross_val_score(model, X_train, y_train, cv=kfold)
print("Accuracy (using KFold) in train data: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

#--------------------------------------
# make class predictions with the model
print('Starting the Prediction phase')
print('#####################################')
#predictions = model.predict_classes(np.asarray(X_Process_test).astype(np.float32))

y_pred = model.predict(X_test)
predictions = [round(value) for value in y_pred]

rmse = np.sqrt(mean_squared_error(y_test, predictions))
print("RMSE: %f" % (rmse))

# predict probabilities for test set
predictions_probs = model.predict(X_test)

print('---------- A sample of 10 prediction ------------')
#summarize the first 10 cases
for i in range(10):
	print('%d (expected %d)' % (np.array(predictions)[i], np.array(y_test)[i]))

    
#print(predictions)
#print(y_test)

print('EVALUATION METRICS with the test sample and prediction data')
print('---------------------------------------------------------------')
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Test data Accuracy: %.2f%%" % (accuracy * 100.0))

#Additonally, evaluate XGBoost Models With k-Fold Cross Validation
kfold = KFold(n_splits=10)
#kfold = KFold(n_splits=10, shuffle=True, random_state=7)
results = cross_val_score(model, X_test, y_test, cv=kfold)
print("Accuracy (using KFold) in test data: %.2f%% (%.2f%%)" % (results.mean()*100, results.std()*100))

# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test, predictions)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test, predictions)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test, predictions)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test, predictions)
print('F1 score: %f' % f1)
 
# kappa
kappa = cohen_kappa_score(y_test, predictions)
print('Cohens kappa: %f' % kappa)

# ROC AUC
auc = roc_auc_score(y_test, predictions_probs)
print('ROC AUC: %f' % auc)

print('Confusion Matrix')
matrix = confusion_matrix(y_test, predictions)
print(matrix)

pyplot.imshow(matrix, cmap=pyplot.cm.hot)
pyplot.xlabel("Predicted labels")
pyplot.ylabel("True labels")
pyplot.xticks([], [])
pyplot.yticks([], [])
pyplot.title('Confusion matrix ')
pyplot.colorbar()
pyplot.show()

#---- OPTUNA framework (minimizing RMSE)----
#def objective(n_trials):
#  params = {
#        "n_estimators":n_trials.suggest_int("n_estimators", 100, 2000, step=100),
#        "learning_rate":n_trials.suggest_float("learning_rate", 1e-4, 0.5, log=True),
#        "reg_alpha":n_trials.suggest_int("reg_alpha", 0, 5),
#        "reg_lambda":n_trials.suggest_int("reg_lambda", 0, 5),
#        "min_child_weight":n_trials.suggest_int("min_child_weight", 0, 5),
#        "gamma":n_trials.suggest_int("gamma", 0, 5),
#        "max_depth": n_trials.suggest_int("max_depth", 3, 25),
#        "colsample_bytree":n_trials.suggest_discrete_uniform("colsample_bytree",0.1,1,0.01),
#        "n_iter_no_change": 50,
#    }
#  dtrain = xgb.DMatrix(data = X_train, label = y_train)
#  dtest = xgb.DMatrix(data = X_test, label = y_test)

#  model = xgb.train(params, dtrain) 
#  y_pred = model.predict(dtest)
#  rmse = mean_squared_error(y_test, y_pred, squared=False)

#  return rmse

# make a study
#study = optuna.create_study(direction="minimize")
#study.optimize(objective, n_trials=500)

#print(f"Optimized RMSE: {study.best_value:.4f}")
#print("Best params:")
#for key, value in study.best_params.items():
#    print(f"\t{key}: {value}")

#The below uses plotly package to plot
#plot_optimization_history(study)

##plot_optimization_history(study) :
##plots optimization history of all trials as well as the best score at each point.
#fig = optuna.visualization.plot_optimization_history(study)
#fig.show()

##plot_slice(study) :
##plots the parameter relationship as slice also we can see which part of search space were explored more.
#fig2 = optuna.visualization.plot_slice(study)
#fig2.show()

##plot_parallel_coordinate(study) :
##plots the interactive visualization of the high-dimensional parameter relationship in study and scores.
#fig3 = optuna.visualization.plot_parallel_coordinate(study)
#fig3.show()

##plot_contor(study) :
##plots parameter interactive chart from we can choose which hyperparameter space has to explore.
##fig4 = optuna.visualization.plot_contor(study)
##fig4.show()

#---- OPTUNA framework (miximazing ACCURACY)----
def objective2(n_trials):
   tree_method = ['exact','approx','hist']
   boosting_list = ['gbtree', 'gblinear', 'dart']
   objective_list_reg = ['reg:squarederror', 'reg:gamma', 'reg:tweedie', 'binary:logistic']
   eval_metric_list = ['rmse', 'mae', 'logloss', 'error', 'merror', 'auc']
   params = {
        "boosting":n_trials.suggest_categorical('boosting', boosting_list),   
        "tree_method":n_trials.suggest_categorical('tree_method', tree_method),
        "max_depth": n_trials.suggest_int("max_depth", 3, 50),
        "reg_alpha":n_trials.suggest_int("reg_alpha", 0, 10),
        "reg_lambda":n_trials.suggest_int("reg_lambda", 0, 10),
        "min_child_weight":n_trials.suggest_int("min_child_weight", 0, 10),
        "gamma":n_trials.suggest_int("gamma", 0, 5),
        "learning_rate":n_trials.suggest_float("learning_rate", 1e-4, 0.5, log=True),
        "eval_metric":n_trials.suggest_categorical('eval_metric', eval_metric_list),
        "objective":n_trials.suggest_categorical('objective', objective_list_reg),
        "colsample_bytree":n_trials.suggest_discrete_uniform("colsample_bytree",0.1,1,0.01),
        "colsample_bynode":n_trials.suggest_discrete_uniform('colsample_bynode', 0.1, 1, 0.01),
        "colsample_bylevel":n_trials.suggest_discrete_uniform("colsample_bylevel",0.1,1,0.01),
        "subsample":n_trials.suggest_discrete_uniform('subsample', 0.5, 1, 0.05),
        "n_estimators":n_trials.suggest_int("n_estimators", 100, 4000, step=100, log=False),
        "max_delta_step":n_trials.suggest_int("max_delta_step", 0, 10),
        "scale_pos_weight": n_trials.suggest_int("scale_pos_weight", 0, 10),
        "n_iter_no_change": 100,
        "nthread" : -1,  
   }
   #earlyStop=20
   dtrain = xgb.DMatrix(data = X_train, label = y_train)
   dtest = xgb.DMatrix(data = X_test, label = y_test)

   #model = xgb.train(params, dtrain, early_stopping_rounds=earlyStop)
   model = xgb.train(params, dtrain) 
   y_pred = model.predict(dtest)
   y_preds = [round(value) for value in y_pred]
   accuracy = accuracy_score(y_test, y_preds)

   # Handle pruning based on the intermediate value.
   if n_trials.should_prune():
      raise optuna.exceptions.TrialPruned()

   n_trials.set_user_attr(key="best_model", value=model) # save model not physically on disk but in memory

   return accuracy

# callback function to save the best model as user attribute
def callback(study, n_trials):
   if study.best_trial.number == n_trials.number:
       study.set_user_attr(key="best_model", value=n_trials.user_attrs["best_model"])

# make a study
study = optuna.create_study(direction="maximize")
study.optimize(objective2, n_trials=5, timeout=None, callbacks=[callback], show_progress_bar = True)
#pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
#complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

print(f"Optimized ACCURACY: {study.best_value:.4f}")
print("Best params:")
for key, value in study.best_params.items():
    print(f"\t{key}: {value}")

# The below uses plotly package to plot
#plot_optimization_history(study)

#plot_optimization_history(study) :
#plots optimization history of all trials as well as the best score at each point.
fig = optuna.visualization.plot_optimization_history(study)
fig.show()


#plot_slice(study) :
#plots the parameter relationship as slice also we can see which part of search space were explored more.
fig2 = optuna.visualization.plot_slice(study)
fig2.show()


#plot_parallel_coordinate(study) :
#plots the interactive visualization of the high-dimensional parameter relationship in study and scores.
fig3 = optuna.visualization.plot_parallel_coordinate(study)
fig3.show()


#Visualize parameter importances.
fig4 = optuna.visualization.plot_param_importances(study)
fig4.show()

#Visualize empirical distribution function
fig5 =  optuna.visualization.plot_edf(study)
fig5.show()

#plot_contour: plots parameter interactions on an interactive chart.
#You can choose which hyperparameters you would like to explore.

##optuna.visualization.plot_contour(study, params=['num_leaves',
##                            'max_depth',
##                            'subsample',
##                            'learning_rate',
##                            'subsample'])

fig6 = optuna.visualization.plot_contour(study)
fig6.show()

# Check this --> https://tech.preferred.jp/en/blog/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization/
####################################################
# Retrieve the best model from Optuna study and plot
best_model = study.user_attrs['best_model']
dtest = xgb.DMatrix(data = X_test, label = y_test)
y_pred = best_model.predict(dtest)

predictions = [round(value) for value in y_pred]
print('EVALUATION METRICS with prediction data after optimizing with the OPTUNA')
print('---------------------------------------------------------------')
# evaluate predictions
accuracy = accuracy_score(y_test, predictions)
print("Test data Accuracy: %.2f%%" % (accuracy * 100.0))
# accuracy: (tp + tn) / (p + n)
accuracy = accuracy_score(y_test, predictions)
print('Accuracy: %f' % accuracy)
# precision tp / (tp + fp)
precision = precision_score(y_test, predictions)
print('Precision: %f' % precision)
# recall: tp / (tp + fn)
recall = recall_score(y_test, predictions)
print('Recall: %f' % recall)
# f1: 2 tp / (2 tp + fp + fn)
f1 = f1_score(y_test, predictions)
print('F1 score: %f' % f1)
 
# kappa
kappa = cohen_kappa_score(y_test, predictions)
print('Cohens kappa: %f' % kappa)
##
# ROC AUC
auc = roc_auc_score(y_test, predictions_probs)
print('ROC AUC: %f' % auc)

print('Confusion Matrix')
matrix = confusion_matrix(y_test, predictions)
print(matrix)

pyplot.imshow(matrix, cmap=pyplot.cm.hot)
pyplot.xlabel("Predicted labels")
pyplot.ylabel("True labels")
pyplot.xticks([], [])
pyplot.yticks([], [])
pyplot.title('Confusion matrix ')
pyplot.colorbar()
pyplot.show()
