#
import os
import warnings
warnings.filterwarnings("ignore")

import numpy as np
import numpy as np_weights

import shap

from numpy import genfromtxt

from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import Dropout

from tensorflow.keras.layers import BatchNormalization

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ModelCheckpoint

from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.optimizers import Nadam
from tensorflow.keras.optimizers import Ftrl
from tensorflow.keras.optimizers import Adagrad
from tensorflow.keras.optimizers import Adamax
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.optimizers import Adadelta
from tensorflow.keras.optimizers import SGD

from tensorflow.keras.utils import plot_model
from matplotlib import pyplot

from tensorflow.keras.models import load_model


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import roc_auc_score

from pandas import read_csv
import pandas as pd

import tensorflow as tf

#The explainer in new version of shap was moved and imported as below
from shap import Explainer

from tensorflow.keras.callbacks import TensorBoard
from time import time

dataset = read_csv('NEW3_permissions_and_intent_vectors_raw_csv.csv', delimiter=';')

# determine the number of input features (columns)
n_features = dataset.shape[1]
print('Number of Features(columns):', n_features)
print('Number of records(lines):', len(dataset))

# split into input (X) and output (y) features
XDataSet = dataset.drop(['is_malware'], axis=1)
yDataset = dataset['is_malware']

X_train, X_test, y_train, y_test = train_test_split(XDataSet, yDataset, train_size=0.80, test_size=0.20)

print("DATA")
print("Train X data shape: ", X_train.shape)
print("Length of X train dataL", len(X_train))

print("Validation X data shape: ", X_test.shape)
print("Length of X test dataL", len(X_test))

print("Train Y data shape: ", y_train.shape)
print("Length of Y train data", len(y_train))

print("Validation Y data shape: ", y_test.shape)
print("Length of Y test data", len(y_test))

#######-- FOR OPTUNA ---########
# First import Optuna

import optuna
from optkeras.optkeras import OptKeras
from optuna.integration import KerasPruningCallback
from optuna.trial import TrialState
from optuna.visualization.matplotlib import plot_optimization_history

N_TRAIN_EXAMPLES = len(X_train)
N_VALID_EXAMPLES = len(X_test)
BATCHSIZE = 25
CLASSES = 2 #binary classification
EPOCHS = 2000
N_TRIALS = 25 #number of trails to run

#Apply the preferred activation function
ACTIVATION = 'sigmoid'
#ACTIVATION = 'softsign'
#ACTIVATION = 'elu'
#ACTIVATION = 'selu'

X_train = X_train.astype("float32")
X_valid = X_test.astype("float32")

# Convert class vectors to binary class matrices.
y_train = keras.utils.to_categorical(y_train[:N_TRAIN_EXAMPLES], CLASSES)
y_test = keras.utils.to_categorical(y_test[:N_VALID_EXAMPLES], CLASSES)


# Helper function to create model and optimize the number of layers, numbers units.

def create_model(trial):

  n_layers = trial.suggest_int("n_layers", 1, 6)
  model = tf.keras.Sequential()
  for i in range(n_layers):
    # num_hidden = the features of each layer
    num_hidden = trial.suggest_int("n_units_l{}".format(i), 4, n_features, log=True)
    model.add(tf.keras.layers.Dense(num_hidden, activation=ACTIVATION))
    model.add(tf.keras.layers.Flatten())
    dropout = trial.suggest_float("dropout_l{}".format(i), 0.2, 0.5)
    model.add(Dropout(rate=dropout))
  model.add(Dense(CLASSES, activation=ACTIVATION))

  # We compile our model with a sampled learning rate.
  lr = trial.suggest_float("lr", 1e-5, 1e-1, log=True)
  # Apply the preferred Optimizer
  #optimizer=Adadelta(lr=lr)
  #optimizer=SGD(lr=lr)
  #optimizer=Adam(lr=lr)
  optimizer=Adamax(lr=lr)
  #optimizer=Adagrad(lr=lr)
  #optimizer=Nadam(lr=lr)
  #optimizer=Ftrl(lr=lr)
  #optimizer=RMSprop(lr=lr)

  # Apply the preferred loss function
  #loss="mean_squared_error"
  #loss='kullback_leibler_divergence'
  #loss='sparse_categorical_crossentropy'
  #loss='kl_divergence'
  loss='binary_crossentropy'  

  model.compile(loss=loss,
         optimizer=optimizer,
         metrics=["accuracy"],)

  #model.compile(loss="binary_crossentropy",
  #      optimizer=tf.keras.optimizers.RMSprop(lr=lr),
  #      metrics=["accuracy"],)

  #model.compile(loss="binary_crossentropy",
  #      optimizer=Adagrad(lr=lr),
  #      metrics=["accuracy"],)

  #model.compile(loss="mean_squared_error",
  #      optimizer=Adagrad(lr=lr),
  #      metrics=["accuracy"],)
  
  #model.compile(loss="mean_squared_error",
  #      optimizer=Adamax(lr=lr),
  #      metrics=["accuracy"],)

  return model


def objective(trial):
  # Clear clutter from previous session graphs.
  tf.keras.backend.clear_session()

  # Generate our trial model.
  model = create_model(trial)

  # Fit the model on the training data.
  # The KerasPruningCallback checks for pruning condition every epoch.
  model.fit(
        X_train,
        y_train,
        batch_size=BATCHSIZE,
        callbacks=[KerasPruningCallback(trial, "val_accuracy")],
        epochs=EPOCHS,
        validation_data=(X_test, y_test),
        verbose=1,)

  # Evaluate the model accuracy on the validation set.
  score = model.evaluate(X_test, y_test, verbose=0)
  return score[1]

# Run optimization
# Finally, create an Optuna study and run the optimization.
study = optuna.create_study(direction="maximize", pruner=optuna.pruners.MedianPruner())
study.optimize(objective, n_trials=N_TRIALS)
pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])
complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])

print("  Study statistics: ")
print("  Number of finished trials: ", len(study.trials))
print("  Number of pruned trials: ", len(pruned_trials))
print("  Number of complete trials: ", len(complete_trials))

print("Best trial:")
trial = study.best_trial

print("  Optimized Accuracy Value: ", trial.value)
#print(f"Optimized Accuracy Value: {study.best_value:.4f}") --> THIS is the same as the previous
print("  Params: ")
for key, value in trial.params.items():
   print("    {}: {}".format(key, value))

print("Total Trials : {}".format(len(study.trials)))
print("Summarizing of trials in Dataframe")
print(study.trials_dataframe())

#Start ploting
fig = optuna.visualization.plot_optimization_history(study)
fig.show()

fig2 = optuna.visualization.plot_slice(study)
fig2.show()

fig3 = optuna.visualization.plot_parallel_coordinate(study)
fig3.show()

#Plot hyperparameter importances
fig4 = optuna.visualization.plot_param_importances(study)
fig4.show()

#Plot the objective value EDF (empirical distribution function) of a study
fig5 = optuna.visualization.plot_edf(study)
fig5.show()

#Plot intermediate values of all trials in a study
fig6 = optuna.visualization.plot_intermediate_values(study)
fig6.show()

fig8 = optuna.visualization.plot_contour(study)
fig8.show()

#######-- END OPTUNA --########
